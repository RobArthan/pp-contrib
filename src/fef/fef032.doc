=TEX
% TQtemplate.tex
\documentstyle[hol1,11pt,TQ]{article}
\def\Hide#1{}
\def\Bool{``$\it{:}bool\,$''}
\makeindex
\TPPproject{DRA FRONT END FILTER PROJECT}  %% Mandatory field
%\TPPvolume{}
%\TPPpart{}
\TPPtitle{Table Computations for SWORD}  %% Mandatory field
\TPPref{DS/FMU/FEF/032}  %% Mandatory field
\def\SCCSversion{$Revision$%
}
\TPPissue{\SCCSversion}  %% Mandatory field
\TPPdate{\FormatDate{$Date$%
}}  %% Mandatory field (with sensible default)
\TPPstatus{Draft}			%% Mandatory field
\TPPtype{Specification}
\TPPkeywords{}
\TPPauthor{R.~D.~Arthan & WIN01}  %% Mandatory field
%\TPPauthors{Name 1&location 1\\Name 2&location 2\\Name 3&location 3}
\TPPauthorisation{R.B.~Jones & HAT Manager}
\TPPabstract{A specification of the table computations allowed in the Front End
implementation of SWORD for the DRA front end filter project RSRE 1C/6130.}
%\TPPabstractB{}
%\TPPabstractC{}
%\TPPabstractD{}
%\TPPabstractE{}
%\TPPabstractF{}
\TPPdistribution{\parbox[t]{4.0in}{%
	HAT FEF File \\
	Simon Wiseman
}}
%\TPPclass{CLASSIFICATION}
\newfont{\icllogo}{icllogo50}
\def\TPPheadlhs{$\vcenter{\halign{##\cr\icllogo ICL\cr}}$}
%\def\TPPheadlhs{}
%\def\TPPheadcentre{}
%def\TPPheadrhs{}
%\def\TPPfootlhs{}
%\def\TPPfootcentre{}
%\def\TPPfootrhs{}

\begin{document}
\TPPsetsizes
\makeTPPfrontpage

\vfill
\begin{centering}

%\bf Copyright \copyright\ : International Computers Ltd \number\year

\end{centering}

\newpage
\section {DOCUMENT CONTROL}
\subsection{Contents List}
\tableofcontents

\subsection{Document Cross References}
\bibliographystyle{fmu}
\bibliography{fmu,fef}

\subsection{Changes History}
\begin{description}
\item[Issue \SCCSversion ({\FormatDate{$Date$%
}}) ] First draft for internal review.
\end{description}

\subsection{Changes Forecast}
The current issue is a partial draft to promote discussion about the approach.
\pagebreak
\section{GENERAL}
\subsection{Scope}
This document gives a formal specification of part of the SWORD Front
End reducing the security properties given in \cite{DS/FMU/FEF/026}
to lower level properties more closely related to the detailed transformations
of \cite{trans}.  It constitutes part of deliverable D12 of work package 3, as
given in the Phase 2 Technical Proposal, \cite{DS/FMU/017}.

Some of the material in this document was previously included in
\cite{DS/FMU/FEF/026}. Now that the formal development has been further
advanced it was felt more appropriate to collect this level of the
treatment into a separate document.

\subsection{Introduction}

\cite{DS/FMU/FEF/026} gives a formal description of an Execution Model
for the Front End Implementation of SWORD. The Execution Model affords
a rather more concrete formulation of the critical requirements on the
major subsystems of SWORD than the architectural model of \cite{DS/FMU/FEF/022}.
In particular, it separates out the critical requirements
(as regards the $select$ query) on the
SSQL Query Transformation Processor of \cite{DS/FMU/FEF/022} so that they
are completely determined by a model of a ``compiler'' for TSQL which
maps a TSQL query onto the function on tables which it computes.

The purpose of this document is to complete the formal treatment for the
Phase 2 work by further reducing the critical requirements on the SSQL
Query Transformation Processor by placing a bound on the allowed TSQL
queries it produces. This is done using what is in effect a reformulation
of the classification computations which underlie the transformations
defined in \cite{trans} in terms of a relational algebra model of TSQL
execution.


\section{PRELIMINARIES}
The following \Product{} instructions set the context for the proof
tools and set up the new theory $fef032$, with parent the theory
$fef026$ in which the Execution Model is defined.
=SML 
open_theory "fef026";
new_theory€"fef032"›;
push_pc "hol";
=TEX
\pagebreak
\section{DISCUSSION}
\subsection{Objectives}
The objective of this document is to define a set of operations on the
derived tables of \cite{DS/FMU/FEF/026} in terms
of which it is possible to characterise the allowable results of
compiling the queries produced by the SSQL Transformation Processor.
This is intended to give some insight into the intuitions about operations
on tables which motivate the transformations. It is also intended to
allow formal reasoning about a reasonably accurate formal model of
the transformations.

\subsection{Formal Approach}

The idea is to identify the primitive operations on tables and on the
security information in them which underlie the semantics of SSQL and
its implementation via the transformations. This amounts to something like
an SSQL analogue of the relational algebra in which security classifcations are
computed along with the data operations. The critical properties
of \cite{DS/FMU/FEF/026} may then be rephrased to say something like
``provided the SSQL Transformation Processor always produces TSQL query
sequences which could equally well have been computed by this SSQL
relational algebra, then the security policy will be enforced''.
Since the computation of security classifications can be formalised
in a way which reflects the syntactic formulation of the transformations
in \cite{trans}, the proviso here is amenable to informal checking, and as
the semantic framework in which these computations are formalised is
much simpler than the syntactic one, their information flow properties
is amenable to formal reasoning.

The definitions of the computations are formally related to the
security policy by using them to characterise a class of table
computations for which the risk inputs, as defined in \cite{DS/FMU/FEF/026},
are either empty or have a known form. This is then used to give a sufficient
condition for the truth of the assertion
=INLINEFT
STP ç STP_secure_E compile
=TEX
\ which plays an important role in \cite{DS/FMU/FEF/026}.
This sufficient condition can be seen to relate more closely, at least informally,
to the internal details of the transformations than the definition
of $STP\_secure$ itself.

\pagebreak
\subsection{Example}

The SSQL analogue of the relational algebra will be formulated
using what is called a ``semantic embedding'', e.g. see\cite{Gordon88},
of the relevant aspects of the SSQL language. It may be helpful to
give a tiny example of this type of approach. For completeness,
the example includes two short proofs, but understanding of these
should not be necessary for the example to motivate the specifications
in the rest of the document.

We consider a language of expressions
formed from constants, $K$, a single variable, $v$, and addition:
=GFT Example Syntax
	E = K | v | E, +, E
=TEX
If the variable is to range over the natural numbers, a semantic
embedding for this little language might go as follows.

First of all, we identify the domain over which the semantic values
of expressions range. In this case, the domain is the set of
functions on the natural numbers taking natural number values;
after some preliminary red tape, we capture this in a type abbreviation

=SML
open_theory"fin_set";
new_theory"eg";
=TEX
=SML
declare_type_abbrev("E", [], î Ó ≠ Ó Æ);
=TEX

We now define the semantic functions for the three constructors
for expressions, arranging to do this so that they compose in a way
which corresponds closely to the abstract structure of the syntax:

πHOLCONST
‹	K : Ó ≠ E;
‹	v : E;
‹	P : E ≠ E ≠ E
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	(µn∑ K n = Ãi∑ n)
‹±	(v = Ãi∑ i)
‹±	(µeâ1 eâ2∑ P eâ1 eâ2 = Ãi∑ eâ1 i + eâ2 i)
∞

So for example, $P\,(K\,1)\,v$ is the semantic value of the expression
$1 + v$.

We may now define the totality of all functions which arise
as the semantics of expressions in the little language; we do
this by forming the smallest set of functions which contains all the
``ground'' functions $K\,i$ and $v$ and is closed under the constructor $P$:

πHOLCONST
‹	A : E 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	A = •{B | (µn∑K n ç B) ± v ç B ± (µfâ1 fâ2∑ fâ1 ç B ± fâ2 ç B ¥ P fâ1 fâ2 ç B)}
∞

Now we can derive an induction principle for the set $A$. Note that
the induction principle gives a means of reasoning by ``induction over
the syntax of the language'' without our having to define the syntax!
=SML
set_pc"hol2";
set_goal([], ¨
	µp:E ≠ BOOL∑
		(µn∑ p(K n))
	±	p v
	±	(µfâ1 fâ2∑ fâ1 ç A ± p fâ1 ± fâ2 ç A ± p fâ2 ¥ p(P fâ1 fâ2))
	¥	(µf∑f ç A ¥ p f)
Æ);
=TEX
The proof of this is an essentially mechanical reformulation
of the definition of $A$ to use properties (boolean functions) rather than sets.
=SML 
a(rewrite_tac[get_spec¨AÆ] THEN REPEAT strip_tac);
a(POP_ASM_T (strip_asm_tac o rewrite_rule[get_spec¨AÆ]
		o µ_elim¨{g | g ç A ± p g}Æ)
	THEN all_asm_fc_tac[] THEN all_asm_fc_tac[]);
val A_induction_thm = save_pop_thm"A_induction_thm";
=TEX
The induction principle (expressed as a HOL theorem) may readily
be turned into a tactic using a standard \Product\ tactic-generating
function:
=SML
val A_induction_tac = gen_induction_tac1 A_induction_thm;
=TEX
This tactic may now be used to reason about the functions in $A$.
For example, we can prove that all of the functions in the set $A$
can be written in a standard form:
=SML
set_goal([], ¨
	µf∑ f ç A ¥ ∂n k∑f = Ãi∑n*i + k
Æ);
=TEX
After applying the induction tactic, this goal reduces to three cases
as one might expect, and the proofs in each case are straightforward:
=SML
a(strip_tac THEN A_induction_tac);
(* *** Goal "1" *** *)
		(* K n = Ãi∑0*i + n *)
a(∂_tac¨0Æ THEN ∂_tac¨nÆ THEN rewrite_tac[get_spec¨KÆ]);
(* *** Goal "2" *** *)
		(* v = Ãi∑1*i + 0 *)
a(∂_tac¨1Æ THEN ∂_tac¨0Æ THEN rewrite_tac[get_spec¨vÆ]);
(* *** Goal "3" *** *)
		(* P(Ãi∑ n*i + k)(Ãi∑ n'*i + k') = (Ãi∑ (n + n')*i + (k + k')) *)
a(∂_tac¨n + n'Æ THEN ∂_tac¨k + k'Æ THEN asm_rewrite_tac[get_spec¨PÆ]
	THEN PC_T1 "lin_arith" prove_tac[]);
val A_standard_form_thm = save_pop_thm"A_standard_form_thm";
=TEX
Now we tidy up by removing the theory in which the example was produced.
=SML
open_theory"fef032";
delete_theory"eg";
=TEX

For what follows, the main feature of the above example is
the use of semantic concepts only to define what one might
otherwise define by a mixture of syntactic and semantic means (``the
set of all computations definable in such-and-such a language'').
We will be concerned with information-flow security properties of a simplified
model of the SWORD implementation. The semantic functions we will define will
bring out the security checks which correspond to each of the constructors
of the SSQL language. The intention is that the overall security checks
required by the SSQL semantics will then reduce to properties of the individual
constructors, in much the same way that the ``standard form'' property
in the example reduces, by an induction principle, to properties of
$K$, $v$ and $P$.
 
\pagebreak
\subsection{Overview}

The plan of what follows is similar in broad outline to the example
of the previous section. However, because we are dealing with a significant
fragment of a real language, the details are quite a lot more complex.

First we must identify the relevant semantic domains.  As in
\cite{DS/FMU/FEF/026}, we wish to simplify matters by ignoring naming
issues. Thus, the rather elaborate means provided by SSQL to name
objects are modelled here by numerical indices of columns in tables, or
of tables within a list of same.  We will also ignore type distinctions
in the syntax, although we will try to point out, informally, places
where type information may be relevant to security. Of course, the
underlying data we work with is the typed data of the SSQL
language as formalised in \cite{DS/FMU/FEF/004}, and so, to that extent,
the treatment does reflect the type system of SSQL.  Furthermore, we
are only concerned with a part of the SSQL language.
Indeed, some of the syntactic sorts of SSQL are not part of the
data manipulation language and are not treated here; moreover, within
the data manipulation language, only the $SELECT$ query is to be covered
by the present work.

After these simplifications, there are really only
two semantically distinct syntactic sorts left, namely {\em values},
denoting the sort of things which appear in the fields of a database
row,, and, {\em tables}, denoting whole tables.  The correspondence
between this simplified view of sorts and that of the SSQL
specification of \cite{specssql} is shown in the following table:

\begin{center}
\begin{tabular}{|l|l|p{4.0in}|}\hline
SSQL & Simplified & Remarks \\
Sort & Sort & \\\hline\hline
$Type$ & n/a & see above \\\hline
$Clause$ & n/a & not part of the DML \\\hline
$Constant\_value$ & VALUE & distinction from $Value$ not relevant here \\\hline
$Value$ & VALUE & --- \\\hline
$Col\_spec$ & n/a & numeric index used instead \\\hline
$Table\_spec$ & n/a & numeric index used instead \\\hline
$Col\_name$ & n/a & numeric index used instead \\\hline
$From\_spec$ & TABLE & distinction from $Tuple\_list$ not relevant here \\\hline
$Target\_spec$ & n/a & not part of the $SELECT$ query \\\hline
$Set\_clause$ & n/a & not part of the $SELECT$ query \\\hline
$Tuple\_list$ & TABLE & distinction from $From\_spec$ not relevant here \\\hline
$Select\_value$ & VALUE & distinction from $Value$ not relevant here \\\hline
$Select\_list$ & TABLE & just a list of $Select\_value$s \\\hline
$Query$ & TABLE & only dealing with $SELECT$ here \\\hline
$BoundQuery$ & n/a & distinction from $Query$ not relevant here \\\hline
\end{tabular}
\end{center}

The type we use for computations of sort $TABLE$ is closely
related to the type used to model the TSQL compiler in \cite{DS/FMU/FEF/026}:
in that document the compiler is thought of as a function which
given a query produces a function on lists of derived tables; the result
returned by the function representing a compiled query is a pair comprising
a new derived table and a list of error indicators. Here, on
the one hand, for simplicity, we assume that error detection in the TSQL
database is disabled (as, indeed, we understand it is likely to be in
the SWORD implementation); on the other hand, we need some additional
information to keep track of a security-relevant issue which is
not currently addressed in \cite{trans}, namely, a classification to
be used when a table computation appears within some other computation,
i.e., a nested $SELECT$. This classification represents the clearance
required to evaluate the $GROUPBY$ and $HAVING$ clauses in the nested
$SELECT$. We arrive at the type given in the following type abbreviation
for the semantics of table computations.

=SML
declare_type_abbrev(€"TABLE_COMP"›, [],
	îDerTable LIST ≠ (Class ∏ DerTable)Æ);
=TEX

The type used for computations of sort $VALUE$, which we will often
refer to as {\em expressions}, is a little bit more
complicated and is forced on us by the SSQL semantics. The simplest
expressions just read some fields in a table row and compute from them a class
and a data item. The set functions operate on a list of rows determined
by the $GROUPBY$ clause in the surrounding table computation.
The $exists\_tuples$ and $single\_value$ expressions operate
on a nested $SELECT$ and so require access to the list of derived tables
which was the operand of the surrounding table computation. We arrive
at the type of functions with three arguments given in the
following type abbreviation for the semantics of value computations:

=SML
declare_type_abbrev(€"VALUE_COMP"›, [],
	î(DerTable LIST ≠ DerTableRow LIST≠ DerTableRow ≠ (Class ∏ Item))Æ);
=TEX

The semantic functions for the value and table  computations
are defined in sections \ref{VALUECOMPUTATIONS} and \ref{TABLECOMPUTATIONS}
respectively. Section \ref{CLOSUREOPERATION} brings
these definitions together to define the algebra of relational operations
which we will claim has the relevant security properties.
Section \ref{CRITICALPROPERTIES} defines some notions relating
the earlier material to the critical properties identified
in \cite{DS/FMU/FEF/026}.

\pagebreak
\subsection{Omissions and Assumptions}\label{OmissionsandAssumptions}
The present version of this document omits certain features of
the transformations of \cite{trans} and relies on various simplifying
assumptions. These shortcomings arise from several practical difficulties,
and it may be helpful to summarise these here:

\begin{enumerate}
\item
The model given here effectively suppresses errors arising in the
evaluation of queries. It is understood that such errors will actually
be suppressed in the SWORD implementation since there are known security
risks associated with them. Where it is necessary to model error cases
explicitly here, it is done by setting result data to an arbitrary but fixed
value. This may be inaccurate in the case of the uniqueness
checks made for the $single\_value$ and $evaluate$ forms (and in the
auxiliary $CommonValue$ used to model part of the semantics of $HAVING$
clauses). Elsewhere, this sanitising is only required in cases
corresponding to compile/transformation-time errors which happen to be
modelled here dynamically, and so is less likely to affect security.
\item
The type conversions ($convert$ value form) have not been dealt with explicitly
in the formal treatment. The
only security-relevant aspect of these is the possibility the conversion
would lead to a run-time error. Since we are assuming that error-detection
is disabled, this value form may be considered to be covered under
the treatment of monadic operators in section~\ref{MonOp}.
\item
The conversions which are parameterised by a table ($convert\_domain$ value form)
are potential covert channels. Unfortunately, the checks which must be
imposed to prevent illicit information flow via this means are not defined
in \cite{trans}. If the conversion domain tables are taken as part of
the fixed structure of the database, then this value form may be
considered to be covered under the treatment of monadic operators
in section~\ref{MonOp}. 
\item
The representation of SSQL data items is borrowed from the Phase 1 work on the
semantics in \cite{DS/FMU/FEF/004}; this uses a different treatment of sterling
and dinary data from \cite{trans} and the more recent issues of \cite{specssql}.
For present purposes, where the later syntax has separate options for selecting
or defining data as sterling or dinary, we have just modelled the default
(sterling) case. It seems unlikely that amending the present specification
to handle the more recent syntax would significantly affect the relevant
security issues.
\item
The $all\_bin\_op$, $some\_bin\_op$ 
value forms and their $\_list$ variants are not catered for.
This is because there were some small problems with the treatment
of these which were remarked upon when \cite{DS/FMU/FEF/020} was being written,
and it was agreed at that time to leave these out.
Remedying these deficiencies would probably be straightforward and introduce
no significant new features as regards security.
\item
The $row\_existence$  value form is omitted from the present draft.
It would appear that this form can always be eliminated in
favour of an equivalent use of a $joined\_row\_existence$
inside a nested $SELECT$ in the $From\_spec$ of a query (this is certainly
true of the model of the language given here, even if syntactic restrictions
prohibit it for SSQL proper). Inclusion of $row\_existence$ would
require a slight complication of the semantic domains to allow the
extra information it requires about the construction
of the current row to be available.
\item
The $context$ and $parameter$ value forms are omitted, since it is assumed
that the relevant information has already been bound into the query being
processed.
\item
The $classify$ and $classify\_default$ value forms, are treated in
\cite{trans} so as to allow a client to regrade a value arbitrarily.
This is not secure. It is understood that the use of these forms is
intended to be restricted to the generation of values for use in
update and insert operations, in circumstances where it has already been
checked whether the client is cleared to know the value in question.
These forms are therefore not properly part of the $SELECT$ query and
have been omitted.
\item
The transformations of \cite{trans} are understood to be too lax with
nested $SELECT$s. The approach taken here is to associate an overall
class with the derived table produced by a nested $SELECT$ and
to let this propagate up into the surrounding expression so that it
can be checked against the client clearance when the computation is complete.
It is believed that this approach cannot be implemented with available TSQL
implementations --- it would most naturally correspond to the transformed
query generating some form of exception if the client was not cleared to
know the result of the nested table computation, but this cannot be achieved.

The approach taken here can be viewed as compatible with a treatment
of nested $SELECT$s in which the transformations attempt to evaluate
the check statically and refuse to allow the query if this static check
fails or if it cannot be evaluated at transformation-time. More explicitly,
such a treatment would handle nested $SELECT$s in much the same way as
top-level ones, but would reject nested $SELECT$s which were such that
the optional check query would be used to enforce security at the top level.
\item
The semantics of the $SELECT\,DISTINCT$ query require duplicate rows
to be eliminated from the result table; the semantics
of the $EVALUATE$ query require an error to be signalled if the expression
being evaluated is not single-valued on each group determined by the
$GROUPBY$ clause. The transformations of \cite{trans} do not, however,
introduce any checks that the client is cleared to have the necessary
calculations performed. This point has not yet been addressed here,
and, while semantic functions for these queries are given following
\cite{trans}, they
are commented out of the construction in section \ref{CLOSUREOPERATION},
since they are known not to be secure.
\item
The treatment here of the optional check query in \ref{CRITICALPROPERTIES}
is still rather implicit (as opposed to the data query
for which a specific bound is given in terms of table computations).
A more explicit treatment of this aspect should be straightforward, but
has been deferred until work on proof has begun to validate the current
treatment of the critical properties.
\end{enumerate}

\pagebreak
\section{VALUE COMPUTATIONS}\label{VALUECOMPUTATIONS}

The particular expression forms which model those of TSQL
are given in sections \ref{DenoteConstant} to \ref{Contents} below.
The classifications follow those assigned in \cite{trans}.
The classification information is taken from $internal\_value\sb{class}$
in \cite{trans} unless otherwise noted below.
The order of the treatment is also taken from $internal\_value\sb{class}$.


=TEX

\subsection{Constant Expression}\label{DenoteConstant}
The function giving constant expressions is parameterised by the classification and value
of the constant.
πHOLCONST
‹	€DenoteConstant›	: (Class ∏ Item) ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µci ∑	DenoteConstant ci = Ãtl rl r∑ ci
∞

\subsection{Monadic}\label{MonOp}
The monadic forms are parameterised here by the actual item computation
to perform and the expression which computes the operand.

πHOLCONST
‹	€MonOp›	: (Item ≠ Item) ≠ VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µf e∑	MonOp f e =
‹	Ãtl rl r∑
‹	let	(c, v) = e tl rl r
‹	in	(c, f v)
∞
\subsection{Binary}\label{BinOp}
The logical binary operators are treated specially. In particular, iterated
$and$s and $or$s are effectively treated by $simplify\sb{ands}$ and
$simplify\sb{ors}$in \cite{trans} as composite operators
on lists of expressions, for the purpose of computing the classification.
We model this directly here (although this makes misnomers of the names).
Thus $BinOpAnd$ and $BinOpOr$ below are parameterised by lists of
expressions to compute the operands to be combined.

We use the following to coerce $Item$s into truth values and vice versa:

πHOLCONST
‹	€ItemBool›	: Item ≠ Bool
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µv∑	ItemBool v = (v = ValuedItemItem(MkValuedItem sterling (BoolVal true)))
∞
πHOLCONST
‹	€BoolItem›	: Bool ≠ Item
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µv∑	BoolItem v = ValuedItemItem(MkValuedItem sterling (BoolVal v))
∞
We use the following to compute iterated conjunctions and disjunctions:
πHOLCONST
‹	€ListAnd›	: Bool LIST ≠ Bool
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µb bs∑	(ListAnd [] § true)
‹±	(ListAnd (Cons b bs) § b ± ListAnd bs)
∞
πHOLCONST
‹	€ListOr›	: Bool LIST ≠ Bool
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µb bs∑	(ListOr [] § false)
‹±	(ListOr (Cons b bs) § b ≤ ListOr bs)
∞
Now, we define $BinOpAnd$ which embodies the algorithm of $simplify\sb{ands}$.
As described in \cite{trans}, the idea is that if the expression evaluates
to $true$, then the client knows that all the operands are $true$ and the class
is the l.u.b. of the classes of all the operands, whereas if the expression evaluates
to $false$ then the client knows that some operand is $false$ and the class
can be taken to be the l.u.b. of the classes of the operands which are
$false$.
πHOLCONST
‹	€BinOpAnd›	: VALUE_COMP LIST ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µel∑	BinOpAnd el =
‹	Ãtl rl r∑
‹	let	cvl = Map (Ãe∑e tl rl r) el
‹	in let	v = ListAnd(Map (ItemBool o Snd) cvl)
‹	in let	makecase(c, u) = if  ItemBool u  then  lattice_bottom  else  c
‹	in	((if  v  then  lubl(Map Fst cvl)  else  lubl(Map makecase cvl)),
‹		 BoolItem v)
∞

Now $BinOpOr$ which embodies the algorithm of $simplify\sb{ors}$.
This is ``dual'' to $BinOpAnd$: if the expression evaluates
to $false$, then the client knows that all the operands are $false$ and the class
is the l.u.b. of the classes of the operands, whereas if the expression evaluates
to $true$ then the client knows that some operand is $true$ and the class
can be taken to be the l.u.b. of the classes of the operands which are
$true$.
πHOLCONST
‹	€BinOpOr›	: VALUE_COMP LIST ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µel∑	BinOpOr el =
‹	Ãtl rl r∑
‹	let	cvl = Map (Ãe∑e tl rl r) el
‹	in let	v = ListAnd(Map (ItemBool o Snd) cvl)
‹	in let	makecase(c, u) =
‹		if ItemBool u then c else lattice_bottom
‹	in	((if  v  then  lubl(Map makecase cvl)  else  lubl(Map Fst cvl)),
‹		 BoolItem v)
∞


The other binary forms use an unoptimised treatment of
classifications and are parameterised by the actual item computation
to perform and expressions to give the operands.

πHOLCONST
‹	€BinOp›	: (Item ≠ Item ≠ Item)
‹		≠ VALUE_COMP ≠ VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µf e1 e2 ∑
‹	BinOp f e1 e2 =
‹	Ãtl rl r∑
‹	let	(c1, v1) = e1 tl rl r
‹	in let	(c2, v2) = e2 tl rl r
‹	in	(c1 lub c2, f v1 v2)
∞
\subsection{Triadic}\label{TriOp}

The triadic forms all use an unoptimised treatment of
classifications and are parameterised by the actual item computation
to perform and three expressions giving the operands.

πHOLCONST
‹	€TriOp›	: (Item ≠ Item ≠ Item ≠ Item)
‹		≠ VALUE_COMP ≠ VALUE_COMP ≠ VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µf e1 e2 e3 ∑
‹	TriOp f e1 e2 e3 =
‹	Ãtl rl r∑
‹	let	(c1, v1) = e1 tl rl r
‹	in let	(c2, v2) = e2 tl rl r
‹	in let	(c3, v3) = e3 tl rl r
‹	in	(c1 lub (c2 lub c3), f v1 v2 v3)
∞
\subsection{Conversions}\label{Convert}
See section \ref{OmissionsandAssumptions}.
\subsection{Sterling}\label{Sterling}
See sections \ref{OmissionsandAssumptions} and \ref{Contents}.
\subsection{Dinary}\label{Dinary}
See sections \ref{OmissionsandAssumptions} and \ref{Contents}.
\subsection{Declaration}\label{Declare}
Since we are constructing a model of the semantics of expressions rather
than the syntax, we do not model declarations.

\subsection{Case Expressions}\label{Case}
The class computation for case expressions
is a little complicated. The idea is that evaluation of a case
expression gives some information about all of the test conditions
up to and including the one (if any) corresponding to the branch
which is actually taken; if the client is not cleared to evaluate one of these
test conditions, then the class of the whole expression is
taken to be the class of the first such condition (which ensures that the client
will not be cleared to see the result), otherwise the class of the
whole expression is taken to be the class of the expression in the branch
which is taken. 

The two sorts of case expression are parameterised by the client clearance,
and expressions and lists of pairs of expressions giving the operands.
Lists of pairs are used rather than pairs of lists as in \cite{trans}
to avoid the anomalous case where the two lists have different lengths
(which is prohibited by the syntactic rules for TSQL in the context of \cite{trans}).

The following two auxiliary functions are used to compute the class for
the $caseVal$ form. They correspond to
the queries $check\_list$ and $check\_test$ computed by
$internal\_value\sb{class}$ in \cite{trans} (with the work performed
by the $limb$ functions expanded out).

πHOLCONST
‹	€CheckList›	: Class ≠ Item ≠
‹			((Class ∏ Item) ∏ (Class ∏ Item)) LIST
‹			≠ Class ≠ Class 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc ti cv cvs elsec ∑
‹	CheckList cc ti [] elsec = elsec
‹±	CheckList cc ti (Cons cv cvs) elsec =
‹	let ((cec, cei), (vec, vei)) = cv
‹	in	if	≥cc dominates cec
‹		then	cec
‹		else if ti = cei
‹		then	vec
‹		else	CheckList cc ti cvs elsec	
∞


πHOLCONST
‹	€CheckTest›	: Class ≠ (Class ∏ Item) ≠
‹			((Class ∏ Item) ∏ (Class ∏ Item)) LIST
‹			≠ Class ≠ Class 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc ti tc cvs elsec ∑
‹	CheckTest cc (tc, ti) cvs elsec =
‹	if	cc dominates tc
‹	then	CheckList cc ti cvs elsec
‹	else	tc
∞
The following function computes the value returned by the $caseVal$ form
of case expression.
πHOLCONST
‹	€CaseValValue›	: Item ≠ ((Class ∏ Item) ∏ (Class ∏ Item)) LIST
‹			≠ Item ≠ Item 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µti cv cvs elsev ∑
‹	CaseValValue ti [] elsev = elsev
‹±	CaseValValue ti (Cons cv cvs) elsev =
‹	let ((cec, cei), (vec, vei)) = cv
‹	in	if	ti = cei
‹		then	vei
‹		else	CaseValValue ti cvs elsev	
∞

πHOLCONST
‹	€CaseVal›	: Class ≠ VALUE_COMP
‹			≠ (VALUE_COMP ∏ VALUE_COMP) LIST
‹			≠ VALUE_COMP ≠ VALUE_COMP 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc tst casevals elseval ∑
‹	CaseVal cc tst casevals elseval =
‹	Ãtl rl r∑
‹	let	(tc, ti) = tst tl rl r
‹	in let	cvs = Map (Ã(c, v)∑ (c tl rl r, v tl rl r)) casevals
‹	in let	(ec, ei) = elseval tl rl r
‹	in let	c = CheckTest cc (tc, ti) cvs ec
‹	in let	v = CaseValValue ti cvs ei
‹	in	(c, v)
∞


The following auxiliary functions is used to compute the class for
the $case$ form. It corresponds to the calculations performed by
the query $c$ in the relevant clause of $internal\_value\sb{class}$
in \cite{trans} (with the work performed by the $limb$ functions expanded out).

πHOLCONST
‹	€CaseC›	: Class ≠
‹			((Class ∏ Item) ∏ (Class ∏ Item)) LIST
‹			≠ Class ≠ Class 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc cv cvs elsec ∑
‹	CaseC cc [] elsec = elsec
‹±	CaseC cc (Cons cv cvs) elsec =
‹	let ((cec, cei), (vec, vei)) = cv
‹	in	if	≥cc dominates cec
‹		then	cec
‹		else if ItemBool cei
‹		then	vec
‹		else	CaseC cc cvs elsec	
∞

The following function computes the value returned by the $case$ form
of case expression.
πHOLCONST
‹	€CaseValue›	: ((Class ∏ Item) ∏ (Class ∏ Item)) LIST
‹			≠ Item ≠ Item 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcv cvs elsev ∑
‹	CaseValue [] elsev = elsev
‹±	CaseValue (Cons cv cvs) elsev =
‹	let ((cec, cei), (vec, vei)) = cv
‹	in	if	ItemBool cei
‹		then	vei
‹		else	CaseValue cvs elsev	
∞

πHOLCONST
‹	€Case›	: Class ≠ (VALUE_COMP ∏ VALUE_COMP) LIST
‹		≠ VALUE_COMP ≠ VALUE_COMP 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc casevals elseval ∑
‹	Case cc casevals elseval =
‹	Ãtl rl r∑
‹	let	cvs = Map (Ã(c, v)∑ (c tl rl r, v tl rl r)) casevals
‹	in let	(ec, ei) = elseval tl rl r
‹	in let	c = CaseC cc cvs ec
‹	in let	v = CaseValue cvs ei
‹	in	(c, v)
∞


\subsection{Set Functions}\label{SetFunctions}
The logical set functions are treated in a similar fashion to
the logical binary operators.

πHOLCONST
‹	€SetFuncAllAnd›		: VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µe ∑	SetFuncAllAnd e =
‹	Ãtl rl r∑
‹	let	cil = Map (e tl rl) rl
‹	in let	(cl, il) = Split cil
‹	in let	ri = ListAnd (Map ItemBool il)
‹	in let	makecase (c, u) = if ItemBool u then lattice_bottom else c
‹	in	if	ri
‹		then	(lubl cl, BoolItem ri)
‹		else	(lubl (Map makecase cil), BoolItem ri)
∞

πHOLCONST
‹	€SetFuncAllOr›		: VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µe ∑	SetFuncAllOr e =
‹	Ãtl rl r∑
‹	let	cil = Map (e tl rl) rl
‹	in let	(cl, il) = Split cil
‹	in let	ri = ListOr (Map ItemBool il)
‹	in let	makecase (c, u) = if ItemBool u then c else lattice_bottom
‹	in	if	ri
‹		then	(lubl (Map makecase cil), BoolItem ri)
‹		else	(lubl cl, BoolItem ri)
∞


The general set functions are parameterised by the operation on
lists or sets of item pairs to be computed and by the expression to
be computed for each row.
πHOLCONST
‹	€SetFuncAll›		: (Item LIST ≠ Item) ≠ VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µf e ∑	SetFuncAll f e =
‹	Ãtl rl r∑
‹	let	(cl, il) = Split (Map (e tl rl) rl)
‹	in	(lubl cl, f il)
∞
πHOLCONST
‹	€SetFuncDistinct›	: (Item SET ≠ Item) ≠ VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µf e ∑	SetFuncDistinct f e =
‹	Ãtl rl r∑
‹	let	(cl, il) = Split (Map (e tl rl) rl)
‹	in	(lubl cl, f (Elems il))
∞

The ``distinct'' option of the logical set functions would appear
to be semantically identical with the ``all'' option and so has
not been given here.
\subsection{Count Functions}\label{Count}
For simplicity, we ignore the numeric type prescription which is
associated with the count functions.

Examination of \cite{trans} reveals that the first
two count functions may be treated
as instances of the general set functions for present purposes, although to
do this we need the function which converts an HOL natural number into
a TSQL item, the precise details being unimportant:


πHOLCONST
‹	€NatItem›	: Ó ≠ Item
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	true
∞

πHOLCONST
‹	€CountNonNull›	: VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µe ∑	CountNonNull e =
‹	let	counter il = NatItem(Length (il ˘ {i | isValuedItem i}))
‹	in	SetFuncAll counter e
∞

πHOLCONST
‹	€CountDistinct›	: VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µe ∑	CountDistinct e =
‹	let	counter is = NatItem(Size (is ° {i | isValuedItem i}))
‹	in	SetFuncDistinct counter e
∞

The $count\_all$ function uses a somewhat different classification
computation taking into account row existence classes.

πHOLCONST
‹	€CountAll›	: VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	CountAll =
‹	Ãtl rl r∑
‹	let	cl = Map DTR_row rl
‹	in	(lubl cl, NatItem(Length rl))
∞

\subsection{$AllBinOp$}\label{AllBinOp}
See section \ref{OmissionsandAssumptions}.
\subsection{$SomeBinOp$}\label{SomeBinOp}
See section \ref{OmissionsandAssumptions}.
\subsection{$ExistsTuples$}\label{ExistsTuples}
This construct is parameterised by a table expression to compute
the operand. The client clearance is also needed to filter out
rows whose existence the client is not cleared to know.
The clearance calculation below is intended to be in the spirit
of $tuple\_list\sb{max\_row\_class}$ from \cite{trans}.
πHOLCONST
‹	€ExistsTuples›	: Class ≠ TABLE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc te ∑
‹	ExistsTuples cc te =
‹	Ãtl rl r∑
‹	let	(c, t) = te tl
‹	in let	trl = DT_rows t ˘
‹			{r | cc dominates DTR_row r ± cc dominates DTR_where r}
‹	in	(c lub lubl (Map DTR_row trl), BoolItem (≥trl = []))
∞
\subsection{$SingleValue$}\label{SingleValue}
This construct is parameterised by a table expression to compute
the operand and the client clearance (to allow rows whose existence
the client is not cleared to see to be hidden).
The classification below is sometimes more generous than that in
$internal\_value\sb{class}$ from \cite{trans} (which uses the statically
known upper bound on the class of the single column in the table).

πHOLCONST
‹	€SingleValue›	: Class ≠ TABLE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc te ∑
‹	SingleValue cc te =
‹	Ãtl rl r∑
‹	let	(c, t) = te tl
‹	in let	trl = DT_rows t ˘
‹			{r | cc dominates DTR_row r ± cc dominates DTR_where r}
‹	in let	cil = DTR_cols (Hd trl)
‹	in let	(ic, ii) = Hd cil
‹	in	if	Length trl = 1 ± Length cil = 1
‹		then	(c lub ic, ii)
‹		else	(c, Arbitrary)
∞

\subsection{Contents}\label{Contents}
Since we have not upgraded to the new SSQL specification we only supply
one contents operator rather than separate sterling and dinary ones.

The contents operator is parameterised by a number telling us which
column to select. We must return a fixed value if this is out of range
(a situation which never arises in the actual SWORD implementation
since the column is identified by name rather than number and it is
known at compile/transformation time whether or not the name is
valid).

πHOLCONST
‹	€Contents›	: Ó ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µi ∑	Contents i =
‹	Ãtl rl r∑
‹	if	1 º i ± i º Length (DTR_cols r)
‹	then	Nth (DTR_cols r) i
‹	else	Arbitrary
∞
\subsection{$Classification$}\label{Classification}
This construct is parameterised by a number telling
us the column whose classification is to be revealed.
The classification below is as in
$internal\_value\sb{class}$ from \cite{trans}.

πHOLCONST
‹	€ClassItem›	: Class ≠ Item
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µv∑	ClassItem v = ValuedItemItem(MkValuedItem sterling (ClassVal v))
∞

πHOLCONST
‹	€Classification›	: Ó ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µi ∑	Classification i =
‹	Ãtl rl r∑
‹	if	1 º i ± i º Length (DTR_cols r)
‹	then	(DTR_row r, ClassItem(Fst (Nth (DTR_cols r) i)))
‹	else	Arbitrary
∞
\subsection{$RowExistence$}\label{RowExistence}
See section \ref{OmissionsandAssumptions}.
\subsection{$JoinedRowExistence$}\label{JoinedRowExistence}
This just returns the row existence field of the current row.
For simplicity, it uses the client clearance, passed as a parameter,
as the result clearance. This is stronger than \cite{trans} in general.
πHOLCONST
‹	€JoinedRowExistence›	:Class ≠  VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc∑	JoinedRowExistence cc =
‹	Ãtl rl r∑ (cc, ClassItem(DTR_row r))
∞
\pagebreak
\section{TABLE COMPUTATIONS}\label{TABLECOMPUTATIONS}
To specify the $SELECT$ and related queries it is convenient to break them
down into more primitive operations: {\em join}, {\em projection} etc.
In this section, we give these top-level building blocks and then combine them in
various ways to describe the $SELECT$ query and its variants.


\subsection{Auxiliary Functions}
\subsubsection{Join}
The TSQL join operation is just cartesian product. The row class
and where clause class in each tuple in the product is the least upper
bound of the corresponding classes in each component tuple which contributes to
that tuple. The joined table has no name and the maximum
row class is the least upper bound of those in the component tables.


Cf. $tuple\_list\sb{make\_outer}$ in \cite{trans}.


πHOLCONST
‹	€JoinSpecs› : DerTableSpec LIST ≠ DerTableSpec
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	µsl∑
‹	JoinSpecs sl =
‹	let	n = []
‹	and	mr = lubl (Map DTS_maxRow sl)
‹	and	csl = Flat (Map DTS_colSpecs sl)
‹	in	MkDerTableSpec n mr csl
∞
πHOLCONST
‹	€JoinRows› : DerTableRow ≠ DerTableRow LIST ≠ DerTableRow LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	µr rs∑
‹	JoinRows r rs =
‹	let	join2 rr = (
‹			MkDerTableRow
‹			(DTR_where r lub DTR_where rr)
‹			(DTR_row r lub DTR_row rr)
‹			(DTR_cols r Î DTR_cols rr))
‹	in	Map join2 rs
∞
πHOLCONST
‹	€JoinData› : DerTableRow LIST LIST ≠ DerTableRow LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	JoinData [] = []
‹±	µtab rest∑
‹	JoinData (Cons tab rest) =
‹	if	rest = []
‹	then	tab
‹	else	let	jrest = JoinData rest
‹		in let	join_blk r = JoinRows r jrest
‹		in	Flat (Map join_blk tab)
∞
The join operation itself is parameterised by the table expressions
which compute the tables to be joined.
πHOLCONST
‹	€Join› : DerTable LIST ≠ (DerTableSpec ∏ DerTableRow LIST)
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µtabl∑	Join tabl =
‹	(JoinSpecs (Map DT_spec tabl), JoinData (Map DT_rows tabl))
∞
\subsubsection{Projection}
The projection operation is parameterised by a list
of functions which compute class-item pairs from a row of a table,
together with column specifications to use for the computed fields.
(Note this is a more general than projection of a single field to form
a one-column table but includes that as a special case).
The operation is required in two flavours, one to support
$all\_tuples$ and $distinct\_tuples$ and one to support $evaluate$.

In the SWORD implementation the column specifications are effectively
computed during the transformations as required. In the formalisation
here, the column specifications are not in fact significant, but have been
left in to allow for possible further development to model some of
the optimisations performed by the transformations.

The operation acts on a table computed by a table expression given as
a parameter. The parameter expressions are expected to assign
appropriate classifications to the resulting fields.
The computed table is anonymous.

Cf. $tuple\_list\sb{make\_outer}$ in \cite{trans}.

πHOLCONST
‹	€ProjectSpec›	: DerColSpec LIST
‹			 ≠ DerTableSpec
‹			 ≠ DerTableSpec
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µsl s∑	ProjectSpec sl s =
‹	MkDerTableSpec [] (DTS_maxRow s) sl
∞
πHOLCONST
‹	€ProjectData›	: DerTable LIST
‹			 ≠ VALUE_COMP LIST
‹			 ≠ DerTableRow LIST LIST
‹			 ≠ DerTableRow LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µtl el gps∑
‹	ProjectData tl el gps =
‹	let	h gp r =	MkDerTableRow
‹				(DTR_where r) (DTR_row r) (Map (Ãe∑e tl gp r) el)
‹	in let	k gp = Map (h gp) gp
‹	in	Flat (Map k gps)
∞
πHOLCONST
‹	€Project›	: DerTableSpec
‹			≠ DerTable LIST
‹			≠ (VALUE_COMP  ∏ DerColSpec) LIST
‹			≠ DerTableRow LIST LIST
‹			≠ DerTable
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µts tl sellist gps∑
‹	Project ts tl sellist gps =
‹	MkDerTable
‹	(ProjectSpec (Map Snd sellist) ts)
‹	(ProjectData tl (Map Fst sellist) gps)
∞
πHOLCONST
‹	€EvalProjectData›	: DerTable LIST
‹			 ≠ VALUE_COMP LIST
‹			 ≠ DerTableRow LIST LIST
‹			 ≠ DerTableRow LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µtl el gps∑
‹	EvalProjectData tl el gps =
‹	let	h gp r =	MkDerTableRow
‹				(DTR_where r) (DTR_row r) (Map (Ãe∑e tl gp r) el)
‹	in let	k gp =
‹		let	results = Map (h gp) gp
‹		in	if	Size(Elems results) = 1
‹			then	Hd results
‹			else	Arbitrary
‹	in	Map k gps
∞
πHOLCONST
‹	€EvalProject›	: DerTableSpec
‹			≠ DerTable LIST
‹			≠ (VALUE_COMP  ∏ DerColSpec) LIST
‹			≠ DerTableRow LIST LIST
‹			≠ DerTable
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µts tl sellist gps∑
‹	EvalProject ts tl sellist gps =
‹	MkDerTable
‹	(ProjectSpec (Map Snd sellist) ts)
‹	(EvalProjectData tl (Map Fst sellist) gps)
∞

\subsubsection{$WHERE$}
The $WHERE$ operation is parameterised, amongst other things,
by a classification to be used to eliminate rows whose existence the
client is not allowed to know.
The classification computations follow $tuple\_list\sb{make\_outer}$ in
\cite{trans}. The local function $w$ computes the class of the where clauses
for each new row. The local function $h$ computes for each row a pair comprising a
truth value, indicating whether the new row is wanted, and a row, being
the row to appear in the result if the row is wanted.

=TEX
πHOLCONST
‹	€Where› : Class
‹		≠ DerTable LIST
‹		≠ DerTableRow LIST
‹		≠ VALUE_COMP
‹		≠ DerTableRow LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µc tl rl e∑
‹	Where c tl rl e =
‹	let	w r = (DTR_where r lub Fst (e tl rl r))
‹	in let	h r = ((ItemBool(Snd (e tl rl r)) ± c dominates DTR_row r),
‹			MkDerTableRow (w r) (DTR_row r) (DTR_cols r))
‹	in	Map Snd (Map h rl ˘ {(t, r) | t})
∞

\subsubsection{$GROUPBY$/$HAVING$}
The $GROUPBY$ and $HAVING$ operations must be treated together.
It is here that the classification produced by a $TABLE\_COMP$ is
computed in anger.

Cf. $tuple\_list\sb{make\_outer}$ in \cite{trans}.

=TEX
To define the grouping part of the operation, we need some list processing
preliminaries. 
The next two functions are parameterised by a function $gpby$ which
is used to decide whether two rows are in the same group. The idea is
that two rows, $x$ and $y$ are in the same group if $gpby\,x=gpby\,y$.
Given a row and a partial list of groups, $PutInGroup$ adds the
row to the appropriate group in the list (creating a new group with only
one row, if necessary).
πHOLCONST
‹	€PutInGroup› :  ('a ≠ 'b) ≠ 'a ≠ ('a LIST) LIST ≠ ('a LIST) LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µgpby x gp gps∑
‹	PutInGroup gpby x [] = [[x]]
‹±	PutInGroup gpby x (Cons gp gps) =
‹	if	gpby x = gpby (Hd gp)
‹	then	Cons (Cons x gp) gps
‹	else	Cons gp (PutInGroup gpby x gps)
∞
$MakeGroups$ uses $PutInGroup$ to do the complete job of organising a list
into groups according to a given $gpby$ function.
πHOLCONST
‹	€MakeGroups›	: ('a ≠ 'b)
‹			 ≠ 'a LIST
‹			 ≠ ('a LIST) LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µgpby x xs∑
‹	MakeGroups gpby [] = []
‹±	MakeGroups gpby (Cons x xs) = PutInGroup gpby x (MakeGroups gpby xs)
∞

The following function is used to supply the $gpby$ parameter to the above.
It ensures that indices which are out of range are mapped to a fixed arbitrary
value to handle securely an error condition which is detected at
compile/transformation time in the SWORD implementation (where names
rather than numbers are used for the columns and the scope rules detect
invalid names).
πHOLCONST
‹	€ListNth›	: Ó LIST ≠ 'a LIST ≠ 'a LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µn nl list∑
‹	ListNth [] list = []
‹±	ListNth (Cons n nl) list =
‹	Cons
‹	(if	1 º n ± n º Length list
‹	then	Nth list n
‹	else	Arbitrary)
‹	(ListNth nl list)
∞

The following is how we analyse the result of the expressions
which appear in $HAVING$ clauses which take the contents of the columns
which appear in the $GROUPBY$ clause. If the (item part of the) expression
parameter gives the same value in each row then that value is returned
classified with the l.u.b. of the classes returned for the rows; otherwise,
a fixed arbitrary value is returned with the same classification
(modelling a run-time error). This effect may be
achieved using the general set function $SetFuncAll$ as follows:

πHOLCONST
‹	€CommonValue›	: VALUE_COMP ≠ VALUE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µe ∑	CommonValue e =
‹	let	pick il =
‹		if	Size (Elems il) = 1
‹		then	Hd il
‹		else	Arbitrary
‹	in	SetFuncAll pick e
∞


The classification computation below is a suggestion/guess on the
basis of current information. The class computed is the least upper
bound of the classes in the columns required to do the grouping
and the classes of the results of the $HAVING$ test in each group.
If the client clearance does not dominate this class then the
optional check query generated by $tuple\_list\sb{make\_outer}$
would return some rows (i.e., the check would fail and the query would
not be allowed to proceed).
Note that when $Group$ is used, rows whose existence the client is not
cleared to know have been filtered out (using $Where$), and
consequently the grouping on class columns does not contribute to the
result class (following $tuple\_list\sb{make\_outer}$).

πHOLCONST
‹	€Group›	: DerTable LIST
‹			≠ DerTableRow LIST
‹			 ≠ Ó LIST
‹			 ≠ Ó LIST
‹			 ≠ VALUE_COMP
‹			 ≠ (Class ∏ (DerTableRow LIST LIST))
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µtl rl gbsterling gbclass having∑
‹	Group tl rl gbsterling gbclass having =
‹	let	gpby row = (ListNth gbsterling (Map Snd (DTR_cols row)),
‹				ListNth gbclass (Map Fst(DTR_cols row)))
‹	in let	gbc row = lubl(ListNth gbsterling (Map Fst(DTR_cols row)))
‹	in let	gps = MakeGroups gpby rl
‹	in let	has_test gp = ((CommonValue having) tl gp Arbitrary)
‹	in let	cl = lubl (Map gbc rl) lub lubl(Map (Fst o has_test) gps)
‹	in let	wanted_gps = gps ˘ {gp | ItemBool(Snd (has_test gp))}
‹	in	(cl, wanted_gps)
∞

\subsection{$TableContents$}\label{TableContents}
This is parameterised by a number giving the index into the list of tables.
The class component of the result is bottom, indicating that no grouping
has yet been done.

πHOLCONST
‹	€TableContents›	: Ó ≠ TABLE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µi∑
‹	TableContents i =
‹	Ãtl∑
‹	if	1 º i ± i º Length tl
‹	then	(lattice_bottom, Nth tl i)
‹	else	Arbitrary
∞
\subsection{$AllTuples$}\label{AllTuples}
πHOLCONST
‹	€AllTuples›	: Class
‹			≠ (VALUE_COMP  ∏ DerColSpec) LIST
‹			≠ TABLE_COMP LIST
‹			≠ VALUE_COMP
‹			≠ Ó LIST
‹			≠ Ó LIST
‹			≠ VALUE_COMP
‹			≠ TABLE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc sellist fromspec where gbsterling gbclass having∑
‹	AllTuples cc sellist fromspec where gbsterling gbclass having =
‹	Ãtl∑
‹	let	(cll, tabs) = Split(Map (Ãte∑te tl) fromspec)
‹	in let	(ts, tab1) = Join tabs
‹	in let	tab2 = Where cc tl tab1 where
‹	in let	(cl, gps) = Group tl tab2 gbsterling gbclass having
‹	in	(cl lub lubl cll, Project ts tl sellist gps)
∞

\subsection{$DistinctTuples$}\label{DistinctTuples}
This is very similar to $AllTuples$; however we need a list-processing
auxiliary to remove duplicate items from a list.

πHOLCONST
‹	€RemoveDuplicates›	: 'a LIST ≠ 'a LIST
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µx xs∑
‹	RemoveDuplicates [] = []
‹±	RemoveDuplicates (Cons x xs)
‹	= Cons x (RemoveDuplicates xs ˘ {y | ≥y = x})
∞

πHOLCONST
‹	€DistinctTuples›	: Class
‹			≠ (VALUE_COMP  ∏ DerColSpec) LIST
‹			≠ TABLE_COMP LIST
‹			≠ VALUE_COMP
‹			≠ Ó LIST
‹			≠ Ó LIST
‹			≠ VALUE_COMP
‹			≠ TABLE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc sellist fromspec where gbsterling gbclass having∑
‹	DistinctTuples cc sellist fromspec where gbsterling gbclass having =
‹	Ãtl∑
‹	let	(cll, tabs) = Split(Map (Ãte∑te tl) fromspec)
‹	in let	(ts, tab1) = Join tabs
‹	in let	tab2 = Where cc tl tab1 where
‹	in let	(cl, gps) = Group tl tab2 gbsterling gbclass having
‹	in let	rem_dups tab = MkDerTable(DT_spec tab)(RemoveDuplicates(DT_rows tab))
‹	in	(cl lub lubl cll, rem_dups(Project ts tl sellist gps))
∞
N.B. the above is known not to be secure
(see section\ref{OmissionsandAssumptions})
and its uses have been commented out later on.
\subsection{$Evaluate$}\label{Evaluate}
Like $DistinctTuples$ this is very similar to $AllTuples$
πHOLCONST
‹	€Evaluate›	: Class
‹			≠ (VALUE_COMP  ∏ DerColSpec) LIST
‹			≠ TABLE_COMP LIST
‹			≠ VALUE_COMP
‹			≠ Ó LIST
‹			≠ Ó LIST
‹			≠ VALUE_COMP
‹			≠ TABLE_COMP
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc sellist fromspec where gbsterling gbclass having∑
‹	Evaluate cc sellist fromspec where gbsterling gbclass having =
‹	Ãtl∑
‹	let	(cll, tabs) = Split(Map (Ãte∑te tl) fromspec)
‹	in let	(ts, tab1) = Join tabs
‹	in let	tab2 = Where cc tl tab1 where
‹	in let	(cl, gps) = Group tl tab2 gbsterling gbclass having
‹	in	(cl lub lubl cll, EvalProject ts tl sellist gps)
∞
N.B. the above is known not to be secure
(see section\ref{OmissionsandAssumptions})
and its uses have been commented out later on.
\pagebreak
\section{CLOSURE OPERATION}\label{CLOSUREOPERATION}

We would now like to form a large set of allowable table computations,
namely, the set of all computations which can be performed by composing
the ones defined above at a given client clearance.
To abbreviate the definition we use the following function
which given a set of pairs of sets, $(a\sb{i}, b\sb{j})$,
returns a pair comprising the intersection of the $a\sb{i}$ and
the intersection of all the $b\sb{j})$.

πHOLCONST
‹	€•â2› : ('a SET ∏ 'b SET) SET ≠ 'a SET ∏ 'b SET
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	µu∑ •â2 u = (•{a | ∂ b∑(a, b) ç u}, •{b | ∂ a∑(a, b) ç u})
∞
Now we give the definition, which is long but straightforward in
its derivation from the various semantic functions. Note that the
last two clauses are commented out, since the relevant semantic functions
as currently formulated above are known not to be secure.
\ftlinepenalty=9999
πHOLCONST
‹	€TableComputations› : Class ≠ TABLE_COMP SET;
‹	€ValueComputations› : Class ≠ VALUE_COMP SET
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc∑
‹	(TableComputations cc, ValueComputations cc) =
‹	•â2 { (tes, es) |
‹		(µci∑ DenoteConstant ci ç es)
‹	±	(µi∑ Contents i ç es)
‹	±	(µi∑ Classification i ç es)
‹	±	CountAll ç es
‹
‹	±	(µf e∑ e ç es ¥ MonOp f e ç es)
‹	±	(µf e1 e2∑ e1 ç es ± e2 ç es ¥ BinOp f e1 e2 ç es)
‹	±	(µf e1 e2 e3∑ e1 ç es ± e2 ç es ± e3 ç es ¥ TriOp f e1 e2 e3 ç es)
‹	±	(µel∑ Elems el Ä es ¥ BinOpAnd el ç es)
‹	±	(µel∑ Elems el Ä es ¥ BinOpOr el ç es)
‹
‹	±	(µcc te cel ee∑ te ç es ± Elems(Map Fst cel) Ä es ±
‹				Elems(Map Snd cel) Ä es ± ee ç es ¥
‹					CaseVal cc te cel ee ç es)
‹	±	(µcc cel ee∑ Elems(Map Fst cel) Ä es ±
‹				Elems(Map Snd cel) Ä es ± ee ç es ¥
‹					Case cc cel ee ç es)
‹
‹	±	(µe∑ e ç es ¥ SetFuncAllAnd e ç es)
‹	±	(µe∑ e ç es ¥ SetFuncAllOr e ç es)
‹	±	(µe∑ e ç es ¥ CountNonNull e ç es)
‹	±	(µe∑ e ç es ¥ CountDistinct e ç es)
‹	±	(µe∑ e ç es ¥ CommonValue e ç es)
‹
‹	±	(µf e∑ e ç es ¥ SetFuncAll f e ç es)
‹	±	(µf e∑ e ç es ¥ SetFuncDistinct f e ç es)
‹
‹	±	(µcc te∑ te ç tes ¥ ExistsTuples cc te ç es)
‹	±	(µcc te∑ te ç tes ¥ SingleValue cc te ç es)
‹
‹	±	(µcc∑ JoinedRowExistence cc ç es)
‹
‹	±	(µi∑ TableContents i ç tes)
‹
‹	±	(µesl tel e1 ml nl e2∑	Elems(Map Fst esl) Ä es
‹			± Elems tel Ä tes ± e1 ç es ± e2 ç es
‹				¥ AllTuples cc esl tel e1 ml nl e2 ç tes)
‹(*	±	(µesl tel e1 ml nl e2∑	Elems(Map Fst esl) Ä es
‹			± Elems tel Ä tes ± e1 ç es ± e2 ç es
‹				¥ DistinctTuples cc esl tel e1 ml nl e2 ç tes)
‹	±	(µesl tel e1 ml nl e2∑	Elems(Map Fst esl) Ä es
‹			± Elems tel Ä tes ± e1 ç es ± e2 ç es
‹				¥ Evaluate cc esl tel e1 ml nl e2 ç tes) *)
‹	}
∞
\pagebreak

\section{CRITICAL PROPERTIES}\label{CRITICALPROPERTIES}
We now wish to relate the above with the critical properties defined
in \cite{DS/FMU/FEF/026} and in particular with the notion of a ``risk input''.
Each table computation as defined in the previous section is a function, $te$ say,
which computes from a list of derived tables, $tl$ say, a pair comprising
a classification, $c$, and a new derived table, $t$.
Restricting attention to the second component of the pair (and tacking
on an empty list of errors) gives us a function to which the
definition of a risk input in \cite{DS/FMU/FEF/026} applies. The intention
is that $tl$ will be a risk input only in cases where the client clearance
does not dominate $c$. This is captured formally by the following
property of table computations.

πHOLCONST
‹	€OkTableComputation› : Class ≠ TABLE_COMP 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcc te∑
‹	te ç OkTableComputation cc §
‹	let	c tl = Fst(te tl)
‹	in let	f tl = (Snd(te tl), [])
‹	in	RiskInputs cc f Ä {tl | ≥cc dominates c tl}
∞

We now say that an SSQL Query Transformation Processor is OK with
respect to a given compiler if the queries it produces compile to
functions which could be computed by an element
of the set of table computations defined in the previous section
and for which the presence and behaviour of the optional check query
relates appropriately to the classification computed by that table computations.
πHOLCONST
‹	€OkSTP›	: (Query ≠ (DerTable LIST ≠ (DerTable ∏ Errors)))
‹		≠ (Query, 'PARS) STP_TYPE 
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹µcompile stp∑
‹	stp ç OkSTP compile §
‹	µq c∑
‹	isError(stp(q, c)) ≤
‹	let	(dq, ocq, pars) = destVal(stp(q, c))
‹	in	is_select (OutL ocq)
‹	±	∂dte∑
‹		dte ç TableComputations c
‹	±	compile dq = (Ãtl∑(Snd(dte tl), []))
‹	±	µtl∑
‹		≥c dominates (Fst (dte tl)) ¥
‹		IsL ocq ± ≥DT_rows(Fst(compile (OutL ocq) tl)) = []
∞
The above says that each output of $stp$ must either indicate
a transformation-time error, or must generate a data query, $dq$, which
could equally well be computed by the table computation $dte$. 
Moreover, in the latter case, if there is any list of derived tables, $tl$,
for which $dte$ would return a class which is not dominated by the
client clearance, then $stp$ must generate a check query and the resulting
check must fail on $tl$. (For simplicity, and because that is the line
taken in \cite{trans}, we actually insist that the check query fail
by producing some rows rather than by causing an error to be signalled.)

We now define two assertions which we conjecture to be true
of the above. The first of these conjectures asserts that the
condition $OkSTP$ is sufficient to ensure the condition $STP\_Secure\_E$
of \cite{DS/FMU/FEF/026}; the second conjecture asserts that the
table computations defined in the previous section do have
the $OkTableComputation$ property.

πHOLCONST
‹	€OkSTP_Secure_E_Lemma› : BOOL
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	OkSTP_Secure_E_Lemma §
‹	µcompile∑
‹		(OkSTP compile: (Query, FILTER_PARS)STP_TYPE ) Ä
‹		STP_secure_E compile
∞
In the above, the type constraint is required, since
for technical reasons connected with the primitive definitional
mechanisms of \Product, it is necessary to constrain the assertion to the
particular type of SSQL Transformation Processors with which we are concerned
rather than leaving the set as a polymorphic one. 
πHOLCONST
‹	€TableComputationsOk_Lemma›
˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
‹	TableComputationsOk_Lemma §
‹	µcc∑ TableComputations cc Ä OkTableComputation cc
∞


\section{CLOSING DOWN}
The following \Product{} instruction restores the previous proof context.
=SML
pop_pc();
=TEX

=IGN
output_theory{theory = "fef032",out_file = "fef032th.doc"};
=TEX

\newpage
\HOLindexOff
\ftlinepenalty=9999
\footnotesize
\input{fef032th.tex}
\HOLindexOn
\newpage
\twocolumn[\section{INDEX} \label{INDEX}]
\small
\printindex
=TEX
\end{document}
=IGN



The set of computations on tables which are allowed to be produced
when queries generates by the SQL Transformation Processor are
compiled will essentially be defined
by taking the closure under composition
of a finite set of particular
operations adequate to represent the primitive constructs of TSQL.
The closure is formed using the following function:

 πHOLCONST
 ‹	€CompClosure›	:	('a LIST ≠ 'a) SET
 ‹			≠	('a LIST ≠ 'a) SET
 ˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
 ‹	µfs∑	CompClosure fs
 ‹	=	•
 ‹		{	a
 ‹		|	fs Ä a
 ‹		±	µh gl il∑
 ‹				h ç a ± Elems gl Ä a ± Elems il Ä 1 .. Length gl
 ‹			¥	let	irl tl = Map (Ãg∑g tl) gl
 ‹				in let	comp tl = h (Map (Nth(irl tl)) il)
 ‹				in	comp ç a}
 ∞
That is to say, the closure under composition of a set of functions, $fs$,
is the smallest set of functions which contains $fs$ and is closed
under the operation which given a function $h$, a list of functions $gl$,
and a list of numbers $il$, returns the function which first applies
each function in $gl$ to produce a list of intermediate results, uses
$il$ as indices to select some or all of the intermediate results in some
order and then applies $h$ to the result. For example, if
=INLINEFT
gl = [gâ1; gâ2; gâ3]
=TEX
\ and
=INLINEFT
il = [3; 1; 1; 2]
=TEX
, the composite function is
=INLINEFT
Ã tl∑ h [gâ3 tl; gâ1 tl; gâ1 tl; gâ2 tl]
=TEX
.
=IGN
PC_C1 "hol1" rewrite_conv(map get_spec[¨MapÆ, ¨NthÆ, ¨LetÆ])
¨let	irl tl = Map (Ãg∑g tl) [gâ1; gâ2; gâ3]
in let	comp tl = h (Map (Nth(irl tl)) [3; 1; 1; 2])
in	compÆ;
=TEX


As with operations on tables, we define the {\em risk inputs} of
an expression $e$ at a class $c$ to be the set inputs for which $e$
reveals information which should not be visible at class $c$.

 πHOLCONST
 ‹	€ExpRiskInputs›	: Class ≠ VALUE_COMP ≠ DerTableRow 
 ˜¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸¸
 ‹µcc e ∑	ExpRiskInputs cc e
 ‹	=	{	r
 ‹		|	∂râ0 ∑
 ‹			HideDerTableRow cc râ0 = HideDerTableRow cc r
 ‹		±	let	(c, i) = e r
 ‹			in let	(câ0, iâ0) = e râ0
 ‹			in	≥i = iâ0
 ‹			±	≥(c dominates cc ± câ0 dominates cc)}
 ∞
